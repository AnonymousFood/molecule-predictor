{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\will\\miniforge3\\lib\\site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import from_networkx\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_connected_subgraph(G, size=4):\n",
    "    \"\"\"Find a connected subgraph of specified size\"\"\"\n",
    "    for component in nx.connected_components(G):\n",
    "        subgraph = G.subgraph(component)\n",
    "        if len(subgraph) >= size:\n",
    "            start_node = np.random.choice(list(subgraph.nodes()))\n",
    "            nodes = list(nx.bfs_tree(subgraph, start_node))[:size]\n",
    "            return nodes\n",
    "    return None\n",
    "\n",
    "def generate_graph(num_nodes=100, edge_prob=0.05):\n",
    "    \"\"\"Generate a random graph ensuring it has at least one connected component of size 4\"\"\"\n",
    "    while True:\n",
    "        G = nx.erdos_renyi_graph(n=num_nodes, p=edge_prob)\n",
    "        connected_nodes = find_connected_subgraph(G, size=4)\n",
    "        if connected_nodes is not None:\n",
    "            return G, connected_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(G, nodes):\n",
    "    \"\"\"Compute graph features including specific node features\"\"\"\n",
    "    if nodes is None or len(nodes) != 4:\n",
    "        raise ValueError(\"Must provide exactly 4 nodes for feature computation\")\n",
    "    \n",
    "    num_nodes = G.number_of_nodes()\n",
    "    if num_nodes == 0:\n",
    "        return torch.zeros(10, dtype=torch.float32)\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # Node-specific features for the provided nodes\n",
    "    for node in nodes:\n",
    "        # Basic metrics\n",
    "        degree = G.degree[node]\n",
    "        clustering = nx.clustering(G, node)\n",
    "        avg_neighbor_degree = np.mean([G.degree[n] \n",
    "                               for n in G.neighbors(node)]) if list(G.neighbors(node)) else 0\n",
    "            \n",
    "        # Centrality metrics\n",
    "        betweenness = nx.betweenness_centrality(G)[node]\n",
    "        closeness = nx.closeness_centrality(G)[node]\n",
    "        pagerank = nx.pagerank(G)[node]\n",
    "            \n",
    "        # Handle eigenvector centrality\n",
    "        try:\n",
    "            eigenvector = nx.eigenvector_centrality_numpy(G)[node]\n",
    "        except (nx.NetworkXError, nx.AmbiguousSolution):\n",
    "            eigenvector = 0\n",
    "            \n",
    "        # Structural metrics\n",
    "        core_number = nx.core_number(G)[node]\n",
    "        local_efficiency = nx.local_efficiency(G)\n",
    "            \n",
    "        node_features = [\n",
    "            degree,\n",
    "            clustering,\n",
    "            avg_neighbor_degree,\n",
    "            betweenness,\n",
    "            closeness,\n",
    "            pagerank,\n",
    "            eigenvector,\n",
    "            core_number,\n",
    "            local_efficiency\n",
    "        ]\n",
    "        features.extend(node_features)\n",
    "    \n",
    "    # Global node-level features (averaged)\n",
    "    degrees = [d for _, d in G.degree()]\n",
    "    clustering_coeffs = [nx.clustering(G, node) for node in G.nodes()]\n",
    "    neighbor_degrees = [np.mean([G.degree[n] for n in G.neighbors(node)]) if list(G.neighbors(node)) else 0 \n",
    "                       for node in G.nodes()]\n",
    "    betweenness = list(nx.betweenness_centrality(G).values())\n",
    "    closeness = list(nx.closeness_centrality(G).values())\n",
    "    pagerank = list(nx.pagerank(G).values())\n",
    "    \n",
    "    # Handle eigenvector centrality for all nodes\n",
    "    try:\n",
    "        eigenvector = list(nx.eigenvector_centrality_numpy(G).values())\n",
    "    except (nx.NetworkXError, nx.AmbiguousSolution):\n",
    "        eigenvector = [0] * num_nodes\n",
    "    \n",
    "    core_numbers = list(nx.core_number(G).values())\n",
    "    \n",
    "    # Calculate global averages\n",
    "    avg_features = [\n",
    "        np.mean(degrees),\n",
    "        np.mean(clustering_coeffs),\n",
    "        np.mean(neighbor_degrees),\n",
    "        np.mean(betweenness),\n",
    "        np.mean(closeness),\n",
    "        np.mean(pagerank),\n",
    "        np.mean(eigenvector),\n",
    "        np.mean(core_numbers),\n",
    "        nx.local_efficiency(G)\n",
    "    ]\n",
    "    \n",
    "    # Global features\n",
    "    global_features = [\n",
    "        nx.density(G)\n",
    "    ]\n",
    "    \n",
    "    # Combine all features\n",
    "    features.extend(avg_features + global_features)\n",
    "    return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "def prepare_node_features(G):\n",
    "    \"\"\"Prepare node features including removal flag\"\"\"\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    # Basic features for each node (5 base features + 1 removal flag)\n",
    "    features = torch.zeros(num_nodes, 6)\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        features[i] = torch.tensor([\n",
    "            G.degree[i],\n",
    "            nx.clustering(G, i),\n",
    "            np.mean([G.degree[n] for n in G.neighbors(i)]) if list(G.neighbors(i)) else 0,\n",
    "            list(nx.betweenness_centrality(G).values())[i],\n",
    "            list(nx.closeness_centrality(G).values())[i],\n",
    "            0  # Removal flag, will be set later\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "def prepare_edge_index(G):\n",
    "    \"\"\"Convert NetworkX graph edges to PyG edge index\"\"\"\n",
    "    return torch.tensor([[e[0], e[1]] for e in G.edges()]).t().contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_graph_data(G, nodes_to_remove):\n",
    "    \"\"\"Process a graph to create training data\"\"\"\n",
    "    # Original graph features\n",
    "    original_features = compute_features(G, nodes_to_remove)\n",
    "    \n",
    "    # Create residual graph\n",
    "    residual_G = G.copy()\n",
    "    residual_G.remove_nodes_from(nodes_to_remove)\n",
    "    \n",
    "    # Get features for residual graph\n",
    "    largest_component = max(nx.connected_components(residual_G), key=len)\n",
    "    residual_G_main = residual_G.subgraph(largest_component)\n",
    "    residual_features = compute_features(residual_G_main, \n",
    "                                       list(residual_G_main.nodes())[:4] if len(residual_G_main) >= 4 else None)\n",
    "    \n",
    "    # Create PyG data object\n",
    "    data = Data(\n",
    "        x=prepare_node_features(G),\n",
    "        edge_index=prepare_edge_index(G),\n",
    "        removed_nodes=torch.tensor(nodes_to_remove),\n",
    "        original_features=original_features,\n",
    "        residual_features=residual_features\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def evaluate_predictions(model, data):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions and print results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "        loss = compute_loss(output, data)\n",
    "        accuracy = calculate_accuracy(output, data)\n",
    "        \n",
    "        print(f\"Evaluation Results:\")\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return output, loss, accuracy\n",
    "\n",
    "def compute_loss(output, data):\n",
    "    \"\"\"Compute MSE loss between predicted and actual features\"\"\"\n",
    "    predicted_features = output.squeeze(0)  # Shape should be [46]\n",
    "    target_features = data.original_features  # Shape should be [46]\n",
    "    \n",
    "    if predicted_features.shape != target_features.shape:\n",
    "        raise ValueError(f\"Shape mismatch: predicted {predicted_features.shape} vs target {target_features.shape}\")\n",
    "    \n",
    "    return F.mse_loss(predicted_features, target_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGNNModel(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim):\n",
    "        super(EnhancedGNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(node_feature_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 9)  # 9 features per node\n",
    "        )\n",
    "        \n",
    "        self.global_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 10)  # Global graph features\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long)\n",
    "        \n",
    "        # Graph convolutions\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "        \n",
    "        # Process features for the 4 selected nodes (9 features each = 36)\n",
    "        node_features = self.node_mlp(x)\n",
    "        selected_nodes = data.removed_nodes\n",
    "        selected_features = node_features[selected_nodes]\n",
    "        selected_features = selected_features.view(-1)  # Flatten to 36 features\n",
    "        \n",
    "        # Global features (10 features)\n",
    "        global_x = global_mean_pool(x, batch)\n",
    "        global_features = self.global_mlp(global_x).squeeze(0)\n",
    "        \n",
    "        # Combine to match target shape (36 + 10 = 46 features)\n",
    "        combined_features = torch.cat([selected_features, global_features])\n",
    "        return combined_features.unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature names to match the 46 output features\n",
    "FEATURE_NAMES = [\n",
    "    # Node-specific features (9 features for each of 4 nodes = 36)\n",
    "    *[f\"Node{i+1}_{metric}\" for i in range(4) for metric in [\n",
    "        'Degree', 'Clustering', 'NeighborDeg', 'Betweenness', \n",
    "        'Closeness', 'PageRank', 'CoreNumber', 'LocalEff', 'Eigenvector'\n",
    "    ]],\n",
    "    \n",
    "    # Global features (10)\n",
    "    'GraphDensity',\n",
    "    'AvgClustering',\n",
    "    'AvgPathLength',\n",
    "    'DegreeAssortativity',\n",
    "    'Transitivity',\n",
    "    'ConnectedComponents',\n",
    "    'MaxDegree',\n",
    "    'MinDegree',\n",
    "    'AvgDegree',\n",
    "    'GlobalEfficiency'\n",
    "]\n",
    "\n",
    "def train_model(model, data, num_epochs=100):\n",
    "    \"\"\"Train the model and track feature-specific accuracies\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    feature_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = compute_loss(output, data)\n",
    "        \n",
    "        # Calculate accuracy for each feature\n",
    "        accuracies = calculate_feature_accuracy(\n",
    "            output.squeeze(0), \n",
    "            data.original_features\n",
    "        )\n",
    "        feature_accuracies.append(accuracies.detach().cpu())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')\n",
    "    \n",
    "    # Average accuracies across epochs\n",
    "    return torch.stack(feature_accuracies).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 9.3241\n",
      "Epoch 10: Loss = 0.8501\n",
      "Epoch 20: Loss = 0.9213\n",
      "Epoch 30: Loss = 1.2720\n",
      "Epoch 40: Loss = 0.6114\n",
      "Epoch 50: Loss = 0.3766\n",
      "Epoch 60: Loss = 0.2356\n",
      "Epoch 70: Loss = 0.5083\n",
      "Epoch 80: Loss = 0.3085\n",
      "Epoch 90: Loss = 0.2549\n",
      "\n",
      "Feature Prediction Accuracies:\n",
      "| Feature             |   Accuracy |\n",
      "|:--------------------|-----------:|\n",
      "| AvgPathLength       |       0.64 |\n",
      "| GraphDensity        |       0.62 |\n",
      "| MinDegree           |       0.56 |\n",
      "| Node3_Degree        |       0.5  |\n",
      "| Node3_NeighborDeg   |       0.49 |\n",
      "| Node4_Degree        |       0.47 |\n",
      "| Node1_LocalEff      |       0.38 |\n",
      "| Node3_LocalEff      |       0.37 |\n",
      "| Node2_NeighborDeg   |       0.29 |\n",
      "| Node4_LocalEff      |       0.29 |\n",
      "| Transitivity        |       0.23 |\n",
      "| Node1_Closeness     |       0.2  |\n",
      "| Node2_Closeness     |       0.2  |\n",
      "| Node3_Closeness     |       0.19 |\n",
      "| Node2_LocalEff      |       0.19 |\n",
      "| Node4_NeighborDeg   |       0.16 |\n",
      "| Node3_CoreNumber    |       0.15 |\n",
      "| Node4_CoreNumber    |       0.15 |\n",
      "| Node4_Closeness     |       0.14 |\n",
      "| Node1_NeighborDeg   |       0.11 |\n",
      "| Node2_CoreNumber    |       0.09 |\n",
      "| GlobalEfficiency    |       0.09 |\n",
      "| MaxDegree           |       0.07 |\n",
      "| AvgClustering       |       0.06 |\n",
      "| Node2_Eigenvector   |       0.05 |\n",
      "| ConnectedComponents |       0.05 |\n",
      "| Node2_Clustering    |       0.05 |\n",
      "| Node1_CoreNumber    |       0.05 |\n",
      "| Node2_Degree        |       0.04 |\n",
      "| Node1_Eigenvector   |       0.04 |\n",
      "| DegreeAssortativity |       0.03 |\n",
      "| AvgDegree           |       0.03 |\n",
      "| Node1_Degree        |       0.03 |\n",
      "| Node3_Eigenvector   |       0.03 |\n",
      "| Node2_Betweenness   |       0.03 |\n",
      "| Node3_PageRank      |       0.02 |\n",
      "| Node4_PageRank      |       0.01 |\n",
      "| Node4_Betweenness   |       0.01 |\n",
      "| Node3_Betweenness   |       0.01 |\n",
      "| Node1_PageRank      |       0.01 |\n",
      "| Node1_Betweenness   |       0.01 |\n",
      "| Node4_Eigenvector   |       0    |\n",
      "| Node4_Clustering    |       0    |\n",
      "| Node1_Clustering    |       0    |\n",
      "| Node3_Clustering    |       0    |\n",
      "| Node2_PageRank      |       0    |\n"
     ]
    }
   ],
   "source": [
    "# Generate graph and prepare data\n",
    "G, selected_nodes = generate_graph(num_nodes=100, edge_prob=0.05)\n",
    "data = process_graph_data(G, selected_nodes)\n",
    "\n",
    "# Initialize and train model\n",
    "model = EnhancedGNNModel(node_feature_dim=6, hidden_dim=64)\n",
    "avg_accuracies = train_model(model, data)\n",
    "\n",
    "assert len(FEATURE_NAMES) == len(avg_accuracies), \\\n",
    "    f\"Feature names ({len(FEATURE_NAMES)}) and accuracies ({len(avg_accuracies)}) must have same length\"\n",
    "\n",
    "# Create results table\n",
    "results_dict = {\n",
    "    'Feature': FEATURE_NAMES,\n",
    "    'Accuracy': [f\"{acc:.4f}\" for acc in avg_accuracies]\n",
    "}\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Display results with nice formatting\n",
    "print(\"\\nFeature Prediction Accuracies:\")\n",
    "print(tabulate(results_df, headers='keys', tablefmt='pipe', showindex=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
